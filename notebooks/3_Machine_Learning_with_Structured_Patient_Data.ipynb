{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOmXUMktrCqz"
   },
   "source": [
    "# ü©∫ Machine Learning with Structured Patient Data\n",
    "\n",
    "In this module, we will work through an illustrated example of using artificial intelligence to develop a patient risk prediction tool using electronic health records (EHR) data. *Adapted from Parisa Rashidi 2021, University of Florida Biomedical Data Science, Department of Biomedical Engineering.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö After this module, students will be able to:\n",
    "\n",
    "* Explain the lifecycle of typical machine learning (ML) projects.\n",
    "* Preprocess datasets using Pandas implementations of common data pipelines.\n",
    "* Train an XGBoost machine learning model using a structured patient dataset.\n",
    "* Build a deep learning (DL) classifier using Keras.\n",
    "* Explain model decision-making through interpretability techniques.\n",
    "\n",
    "In this notebook, we will implement two machine learning models to predict the risk of acute kidney injury (AKI) following major inpatient surgery, using a tabular dataset of patient features.\n",
    "\n",
    "# üöÄ Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MVt0bkArtGu"
   },
   "source": [
    "### ü§î Motivation\n",
    "\n",
    "* Predicting surgical complications *before* surgery can inform (1) shared decisions regarding the appropriateness of procedures, (2) targeted risk-reduction strategies, and (3) postoperative resource use. \n",
    "\n",
    "* Cognitive and judgment errors are major sources of potentially preventable complications.\n",
    "    * Example: if the risk of complications is underestimated, high-risk patients may be triaged to general wards rather than more appropriate intensive care units (ICUs).\n",
    "    \n",
    "* AI-based clinical decision support has the potential to mitigate harm from cognitive errors occurring when estimating the risk of postoperative complications.\n",
    "\n",
    "* All patients have a unique risk profile that is specific to their demographic characteristics, comorbid conditions, physiological reserve, planned surgical procedure, and surgeon skill.\n",
    "\n",
    "* Clinicians sometimes display moderate ability to estimate risk probabilities. AI tools are intended to augment decision-making.\n",
    "\n",
    "* Many clinicians are hindered by time-consuming manual data entry requirements and lack of integration with clinical workflow.\n",
    "    \n",
    "* AI predictive models using automated EHR data inputs may be able to mitigate these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Lifecycle of ML\n",
    "\n",
    "Shown below are the major steps that are involved in a typical ML lifecycle.\n",
    "<img src=\"https://github.com/gatorai/aicc23/raw/main/graphics/ML.png\" alt=\"ML lifecycle\" width=\"50%\">\n",
    "\n",
    "#### 1. Data extraction and processing\n",
    "* Relevant data must be extracted (example: all revelant laboratory values and vital signs).\n",
    "* Data quality and conformation to modeling requirements must be ensured through data preprocessing (example: fixing the missing data, removing any unwanted outliers, and encoding the categorical data.)\n",
    "\n",
    "#### 2. Model development\n",
    "* Machine learning models are built from data.\n",
    "* Recall our module on functions: an ML model can be thought of as a **function** that maps a set of input values (example: patient characteristics) to an output value (example: probability of developing AKI).\n",
    "* We develop a model on data from a **training** cohort.\n",
    "* During the model refinement stage, known as **training**, model parameters (such as *weights* and *biases*) are altered to optimize the associations between inputs and outputs.\n",
    "* After training, a model is ready to make predictions on *new data it has not seen before*. But how accurate will it be?\n",
    "\n",
    "#### 3. Evaluation and Intepretation\n",
    "* Once a ML model is trained, we evaluate the end result on **testing** dataset to see how well the model performed or how reliable it is.\n",
    "* Multiple performance metrics such as accuracy, AUC, sensitivity, and specificity can be evaluated.\n",
    "* Once we are satisfied with a model's performance, we may implement **explainable AI (XAI)** techniques to discover **why** a model behaves in a particular way.\n",
    "* XAI techniques include computing a relative importance score associated with each input feature/column (for a given patient, how important was each input in determining that patient's prediction?).\n",
    "\n",
    "#### 4. Deployment\n",
    "* ML model deployment is the process of placing a finished ML model into a live environment, such as creating a web service or mobile application for prediction.\n",
    "* It is imperative that the performance of an established model is continually monitored in prospective deployment to safeguard against data distribution shifts resulting from changes in populations, practice, or other changes that may result in deteriorating model performance over time.\n",
    "\n",
    "#### 5. Optimization\n",
    "* A deployed ML model has real-world impact and must be continuously monitored and maintained.\n",
    "* When new or updated data becomes available, or when we determine that an existing deployed model no longer results in satisfactory performance, we need to optimize the model by re-training and re-evaluating the ML model.\n",
    "\n",
    "In this notebook, we will only focus on the first three phases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Data and problem definition\n",
    "\n",
    "We will be using an EHR dataset containing 10,000 surgical patients.\n",
    "* **üì• Input features (X)** (47)\n",
    "    * Demographic variables\n",
    "    * Admission and surgery information\n",
    "    * Social determinants of health (SDOH)\n",
    "    * One-year medication history\n",
    "    * Admission comorbidities\n",
    "    * Estimated glomerular filtration rate (eGFR: measures kidney function) before surgery\n",
    "* **üì§ Outcome (Y)** (1)\n",
    "    * Postoperative AKI: onset of acute kidney injury (AKI) at any point between the end of surgery and patient discharge or death.\n",
    "    * This is a binary outcome (it is recorded as either 0 or 1 for each patient). The ML model will learn to predict the risk of each patient developing AKI based on the specific input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('') # Files removed from public access\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° As an exploratory step, let's take a look at the distribution of our AKI outcome (how prevalent is postoperative AKI?). We'll use the `value_counts` function we saw before to help answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The normalize=True parameter divides each count by the total number of samples.\n",
    "df['aki_surg_disch'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç We can see from the above that about 16% of the surgical encounters resulted in acute kidney injury."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Data preprocessing\n",
    "\n",
    "Before we use our data to train a machine learning model, we must transform the raw data into **features** that an ML model can understand (sometimes these look quite similar, but not always). This critical step is called **data preprocessing**. Since real-world data can be messy, during this phase we also *clean* data to ensure the ML model can learn meaningful patterns.\n",
    "\n",
    "Common data preprocessing steps may include the following (depending on the ML model used):\n",
    "* Removing outliers*\n",
    "* Addressing data missingess\n",
    "* Transforming binary string columns (e.g., sex) into a numerical form that the model can understand.\n",
    "* Transforming many-valued string columns (e.g., race) into a numerical form.\n",
    "* Normalizing input features to give them equal weighting.\n",
    "\n",
    "\\*One common approach to remove outliers in ML projects is to remove observations that fall outside a specified range. In this tutorial, we remove observations whose values are less than the 1st percentile or greater than the 99th percentile.\n",
    "\n",
    "\n",
    "### üïµÔ∏è‚Äç‚ôÇÔ∏è Addressing missing data\n",
    "\n",
    "* Missing data are common in routinely collected health data\n",
    "    * Many times, missingness can be informative (e.g. a particular test was not ordered because patient was healthy).\n",
    "    * In other cases, missingness may be at random or due to other factors (e.g., lack of routine follow-up)\n",
    "* Some models **require** filling in missing data with imputation methods. Others will gladly accept missing data. The decision of when, and how, to impute missing data is often dependent on the model, data, and problem. \n",
    "* In a Pandas DataFrame, missing values will be represented by `NaN` (*\"Not a Number\"*)\n",
    "\n",
    "One simple, but effective, imputation approach is to replace a patient's missing variable value with a value computed from all other patients. For example, if a patient is missing the `bmi` variable, we might impute the mean value for the `bmi` feature across all other patients. (Recall our Pandas module for other choices!)\n",
    "\n",
    "**üîç Example:** Let's use code to replace any missing `bmi` values with the `median` bmi from all other patients. We can accomplish this using some Pandas functions we've already seen, along with the `df.fillna(x)` function, which for a given DataFrame `df`, replaces any missing values with the input value `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use Pandas filtering to see examples of where bmi is missing. (Notice the NaN)\n",
    "df[pd.isnull(df['bmi'])].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the median BMI (note: missing values are ignored in this calculation)\n",
    "bmi_median = df['bmi'].median()\n",
    "print(bmi_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fillna to replace any missing BMI values with the median BMI value.\n",
    "df['bmi'] = df['bmi'].fillna(bmi_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-run this code to see where BMI is still missing\n",
    "df[pd.isnull(df['bmi'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ BMI is no longer missing! Every patient now has a BMI value for passing into the model.\n",
    "\n",
    "**üìå Note:** There are many alternative approaches for missing data imputation, including k-Nearest neighbors (KNN) and multiple imputation by chained equation (MICE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚û°Ô∏è Data Transformation\n",
    "In general, machine learning models are great at learning from **numerical** data (e.g., 0, 1, 0.5) . However, many important clinical characteristics are stored as **categorical** data (e.g., 'MALE', 'DIVORCED'). For many models, we must convert the categorical values to numerical features through a **data transformation** process.\n",
    "\n",
    "For categorical variables with **only two** options, we can assign one of the options to be **0**, and the other to be **1**.\n",
    "\n",
    "Let's see an example of how we would do this for the `sex` variable, which in our particular dataset contains only two values: `MALE` and `FEMALE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the unique values of the sex variable before transformation\n",
    "print(df['sex'].unique())\n",
    "\n",
    "# We can use the replace function to convert strings to binary numbers\n",
    "df['sex'] = df['sex'].replace({'FEMALE' : 0, 'MALE' : 1})\n",
    "\n",
    "# Show the unique values of the sex variable after transformation\n",
    "print(df['sex'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Great! But what if our categorical variable contains more than two unique values?\n",
    "* One option would be to replace them with more numbers (for example: 0, 1, 2, 3, ...)\n",
    "* However, doing this may confuse some models due to implicit ordering and hierarchies\n",
    "    * If 0=apple, 1=banana, 2=pear, 3=tomato, then does a tomato equal three bananas? Is a pear worth more than a banana which is worth more than an apple? \n",
    "\n",
    "üìå One solution is called **one-hot encoding**:\n",
    "1. For a single categorical variable (e.g., `marital_status`), create a separate column in the dataset for each category of a categorical variable.\n",
    "    * If there are 4 options, then we create 4 new columns.\n",
    "2. The values of these columns are zero **except for** the column corresponding to the original value.\n",
    "\n",
    "üîç Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 5 values of marital_status before one-hot encoding\n",
    "print(df['marital_status'].head())\n",
    "\n",
    "# Use the get_dummies() function to perform one-hot encoding on the marital_status column.\n",
    "marital_status_transformed = pd.get_dummies(df['marital_status'])\n",
    "\n",
    "# Show the first 5 values of marital_status after one-hot encoding\n",
    "print(marital_status_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è≠Ô∏è Let's skip ahead.\n",
    "Data analysis, imputation, transformation, and preprocessing take a large portion of any AI/ML project. We only covered a few steps of the process.\n",
    "\n",
    "üïë In in the interest of time, let's load a new dataset that has already been preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('') # Files removed from public access\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to train our first machine learning models!\n",
    "\n",
    "We're going to look at two different models to predict postoperative AKI from our patient characteristics contained in `df`:\n",
    "* **XGBoost:** A popular tree-based model that is known to work well with tabular datasets like ours.\n",
    "* **Multi-layer perceptron (MLP):** A simple artificial neural network (ANN) model. This model is considered to be one of the simplest forms of **deep learning (DL)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™ö Splitting the dataset\n",
    "\n",
    "* **Recall** that ML models adjust their internal parameters by feeding in many examples of inputs $X$ and corresponding output $y$. This is called the **training** process. With enough time and training examples, the model can learn associations between inputs and outputs that can help it make a prediction on new data it hasn't ever seen before.\n",
    "\n",
    "* To evaluate how good our ML model is, we will **test** it on a set of examples that it has never seen before (known as the **test set**), and for each patient in the test set, compare the model's predicted outcome $\\hat{y}$ with the true patient outcome $y$.\n",
    "\n",
    "* With enough testing examples, we can calculate performance metrics such as accuracy and other diagnostic metrics like AUC, sensitivity, and specificity.\n",
    "\n",
    "ü™õ Let's take our dataset and prepare it for building our ML model by doing a few final preprocessing items:\n",
    "* Convert the pandas DataFrame into two NumPy arrays: `X` (inputs) and `y` (AKI outcome)\n",
    "* Split X and y into training and testing sets. We'll have four variables after this step:\n",
    "    * `X_train` and `y_train`: the training set\n",
    "    * `X_test` and `y_test`: the testing set\n",
    "    * We'll arbitrarily use 20% of the dataset for testing.\n",
    "\n",
    "‚ö†Ô∏è Some of this code may seem unfamiliar to you. That's OK! Follow along with the comments to understand each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # We'll use this function to create the training and testing sets\n",
    "\n",
    "# Build X (an array/matrix) by taking the values from the entire df DataFrame EXCEPT FOR the first (patient id) and last (AKI outcome) columns\n",
    "X = df.iloc[:, 1:-1].values\n",
    "\n",
    "# Build y (a one-dimensional array) by taking only the final column ('aki_surg_disch'). The order of these outcomes exactly matches the order of the rows in X.\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split X and y into training and testing sets, by using a random 20% of the dataset for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≤ Model 1: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's train our first XGBoost model to predict postoperative AKI from our patient features.\n",
    "* We're going to use the popular Python library `xgboost`, which makes it very easy to train an ML model ***in a single line of code!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create our XGBoost prediction model and assign it to variable xgb\n",
    "xgb = xgb.XGBClassifier()\n",
    "\n",
    "# Train the model using the training dataset\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ That was easy! But is this model any good?\n",
    "\n",
    "To evaluate the performance of our model, we need to compare two quantities:  \n",
    "* $y$, the true outcomes of our testing set patients. In our code, we'll call this list `y_true`.\n",
    "* $\\hat{y}$, our model's **predicted** outcomes for the testing set patients. We'll call this `y_pred` (predicted).\n",
    "\n",
    "üìå The XGBoost model has an easy function called `score` that computes the accuracy on a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show our model's accuracy on the test set.\n",
    "xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Feature importance\n",
    "* We now have our trained XGBoost model, and know how accurate it is at making predictions, but we don't understand **why** it predicts certain outcomes for certain patients.\n",
    "* For this, we need to turn to the field of **explainable AI (XAI)**, an emerging and active field of AI research.\n",
    "* In this module, we'll examine one of the simplest forms of explainability, which looks at the model's weight it assigns to each feature.\n",
    "* Every XGBoost model has a built-in *attribute* variable for this: `feature_importances_` (for convenience, we'll reassign the feature importances to a variable `F`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importances\n",
    "F = xgb.feature_importances_\n",
    "\n",
    "# Take a look at the feature importances.\n",
    "# The scores are in the order of the columns of our input data.\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To make this more informative, we can map these scores to their original feature names and generate a plot to visually compare the importance of each feature.\n",
    "* Let's do this for the model's top 10 features.\n",
    "\n",
    "**‚ö†Ô∏è Note:** Some of this code will be new. It's OK to just follow along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_df = pd.DataFrame({'feature': df.columns.tolist()[1:-1], 'score': F}).sort_values(by='score').tail(10)\n",
    "F_df.plot.barh(x='feature', y='score', figsize=(5,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By examining the model's learned weights, we can see that the most influential input was the `surgery_type` feature (further analysis can reveal which surgery types were most predictive of AKI)\n",
    "* The second most important feature was `lytes`, a comorbidity indicating fluid and electrolyte disorders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Model 2: Artificial Neural Network (Multi-Layer Perceptron)\n",
    "* An important part of an AI research project is evaluating different models to identify which works best for a given dataset and task.\n",
    "* Let's train a different machine learning model on the same dataset.\n",
    "* This time, we'll be using a deep learning (DL) model that uses an artificial neural network called the multi-layer perceptron (MLP).\n",
    "* We will be using the user-friendly Python library `Keras` to build our MLP model.\n",
    "\n",
    "**‚ö†Ô∏è Note:** A full description of DL or the MLP is beyond the scope of this lesson. We refer interested students to [Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press](https://www.deeplearningbook.org).\n",
    "\n",
    "üìå Important bullet points for this module:\n",
    "* Deep learning was biologically inspired.\n",
    "* A DL model like the MLP is often defined in terms of **layers** (one *input layer*, one or more *hidden layers*, and one *output layer*). The more hidden layers that are added to a model, the \"deeper\" it gets.\n",
    "* Each hidden layer is designed to transform the data from the layer before.\n",
    "* The final layer learns to predict an outcome based on all of these transformations.\n",
    "\n",
    "<img src=\"https://github.com/gatorai/aicc23/raw/main/graphics/ANN.jpg\" alt=\"ANN\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõ†Ô∏è Let's create our first MLP model with the following specification:\n",
    "* One input layer with 47 units (one per input variable)\n",
    "* One hidden layer with 128 units\n",
    "* Another hidden layer with 64 units\n",
    "* An output layer with 1 unit (for predicting 0 or 1 corresponding to our AKI outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# Add the first hidden layer with 128 neurons (and implicitly create the input layer by specifying the \"input_dim\")\n",
    "model.add(layers.Dense(units=128, input_dim=47, activation='relu'))\n",
    "\n",
    "# Add the second hidden layer with 64 neurons\n",
    "model.add(layers.Dense(units=64, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model for a classification problem such as ours.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 4016,
     "status": "ok",
     "timestamp": 1654527395770,
     "user": {
      "displayName": "Benjamin Shickel",
      "userId": "07487924651552066401"
     },
     "user_tz": 240
    },
    "id": "BG5sFXLvsi5B"
   },
   "source": [
    "### üí¨ (Optional) Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hidden layer of our neural network will be created using the **Dense** class from Keras. For each layer, we must define the number of hidden units (also known as neurons). There are several optional arguments we may also pass, which can be viewed in the [Keras documentation page](https://keras.io/api/layers/core_layers/dense/). We can add many layers to our deep learning model using the .add() function of the Sequential class. You can think of a Sequential container as a list of hidden layers.\n",
    "\n",
    "For the first layer of our neural network, we must tell Keras how many variables to expect in each input vector. From our previous data exploration, we know that each patient is defined by 47 different variables, so the input dimension to our network is 47.\n",
    "\n",
    "One reason why deep learning models are so powerful is their ability to model complex variable interactions through nonlinear activation functions. We have several choices for activation function. In our example, we will use the commonly chosen Rectified Linear Unit activation (ReLU).\n",
    "\n",
    "Once we are satisfied with the hidden layers of our model, we need to add an output layer for generating class predictions. Our output layer will also be a Dense layer, but it will only have a single (1) unit. Instead of ReLU, we will use a sigmoid activation function, which is typically chosen for binary classification problems such as ours. Using a sigmoid activation on our output layer allows us to interpret the output as a prediction probability. In other words, the probability that a given input vector belongs to class 1.\n",
    "\n",
    "Now that we have defined the architecture of our neural network, we will use the .compile() function to build it. In our example we are defining a few arguments that are associated with the training of our model:\n",
    "* We are using a binary cross-entropy loss. This is an appropriate choise for binary classification.\n",
    "* We will be using the Adam optimizer, which is a popular version of stochastic gradient descent (SGD).\n",
    "* For this example, we are interested in our model's prediction accuracy, so we'll tell Keras to use the \"accuracy\" metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è‚Äç‚ôÇÔ∏è Training our MLP model\n",
    "\n",
    "* Now it's time to train our prediction model!\n",
    "* We will train (also called \"fit\") the model using our training dataset we already created.\n",
    "* We will use the one-line function `.fit()` to train our entire deep learning model.\n",
    "* We will specify some additional parameters to be used during the training process:\n",
    "    * We will tell Keras to train the model for 10 epochs.\n",
    "    * We will use a batch size of 64 samples. During each epoch, the model will pass in 64 samples at a time.\n",
    "    * We will use a random 30% of the training dataset as our **validation set** (different from the test set) for computing metrics while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez-TWmw5uGNT"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1654527396881,
     "user": {
      "displayName": "Benjamin Shickel",
      "userId": "07487924651552066401"
     },
     "user_tz": 240
    },
    "id": "wqFjAlvrlK2R"
   },
   "source": [
    "**üéâ Done!**\n",
    "\n",
    "* Let's check the performance of our trained model on the test set we already created.\n",
    "* The model has never seen this particular data, so it can provide an idea of how well the model might perform in the future (***generalizability to unseen data***).\n",
    "* We will use the Keras function `.evaluate()`, which will compute the loss, as well as any metrics we defined when compiling our model.\n",
    "    * Since we told Keras to use \"accuracy\" when we compiled the model, we will see the model's accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚úèÔ∏è Excercise:** Create an MLP with **3** hidden layers (instead of 2), with the following number of neurons in each:\n",
    "* Hidden layer 1: 512 neurons\n",
    "* Hidden layer 2: 256 neurons\n",
    "* Hidden layer 3: 128 neurons\n",
    "\n",
    "Create, compile, train, and evaluate this new MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it! (feel free to re-use most of the code from before!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚úèÔ∏è Excercise:** Create an MLP with **8** hidden layers (instead of 2), with **any number** of neurons in each hidden layer. Create, compile, train, and evaluate this new MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "MLP-Teacher.ipynb",
   "provenance": [
    {
     "file_id": "1HeKPEhTAMN0YCDHRP9wcYv18UUaaOJQW",
     "timestamp": 1654466786319
    },
    {
     "file_id": "148jGrKocKC_DFagQW4JBd7jtGQnOR9yf",
     "timestamp": 1654462810571
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
